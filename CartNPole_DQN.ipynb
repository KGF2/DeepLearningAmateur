{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CartNPole_DQN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP2D5KpJMoJ/OvyFFLi8smR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KGF2/DeepLearningAmateur/blob/Test/CartNPole_DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rttQEAehDsX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SOURCE: https://www.youtube.com/watch?v=PyQNfsGUnQA&list=PLZbbT5o_s2xoWNVdDudn51XM8lOuZ_Njv&index=16\n",
        "\n",
        "# DQN using PyTorch\n",
        "\n",
        "# import libraries\n",
        "\n",
        "%matplotlib inline\n",
        "import gym\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple\n",
        "from itertools import count\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mm7vJNbwvVYy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  setup display\n",
        "\n",
        "is_ipython = 'inline' in matplotlib.get_backend() # for interactive display of plot\n",
        "if is_ipython : from IPython import display\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAx6yUHZw1br",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Deep Q network using PyTorch\n",
        "\n",
        "class DQN(nn.Module): # Module class is the base class for all NNs. class DQN extends from this module. DQN will recieve screenshoot like images from the Module\n",
        "  def __init__(self, img_height, img_width): # class constructor\n",
        "    super().__init__()\n",
        "\n",
        "    # Network with 2 fully connected hidden layers and an output layer\n",
        "    self.fc1 = nn.Linear(in_features=img_height*img_width*3, out_features=24) # 3 corresponds to 3 RGB input images. flattened/linearised inputs at the input stage.\n",
        "    self.fc2 = nn.Linear(in_features=24, out_features=32)\n",
        "    out = nn.Linear(in_features=32, out_features=2) # 2 since only available actions for the cartpole are Left and Right\n",
        "\n",
        "  def forward(self, t): # network forward propagation, t is the image tensor\n",
        "    t = t.flattened(start_dim=1) # flattenning the image tensor\n",
        "    t = F.relu(self.fc1(t)) # t is passed to the 1st connected layer and RELU is applied as the activation unit\n",
        "    t = F.relu(self.fc2(t))\n",
        "    t = self.out(t)\n",
        "    return t\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N63pGa2v36Q8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  # experiences from replay memory is used to train the network\n",
        "  # to create experiences, we create an Experience class which is used to create instances of experience objects that will get stored in and sampled from replay memory later\n",
        "  Experience = namedtuple('Experiences', ('state', 'action', 'reward', 'new_state'))\n",
        "  #e = Experience(1,2,4,5)   # Eg Object of an experience class\n",
        "  #print(e) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdAmkoPk37XE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# replay memory class to store the above experiences\n",
        "\n",
        "class ReplayMemory():\n",
        "  def __init__(self, capacity): # capacity of the Replay memory, taken as argument\n",
        "    self.capacity = capacity # initialise RMs capacity\n",
        "    self.memory = [] # define a memory attribute equal to an empty list. this is the structure that holds the stored experiences\n",
        "    self.push_count = 0 # to keep track of number of exs added to mem\n",
        "\n",
        "  # to store experiences in RM\n",
        "  def push(self, experience):\n",
        "    if len(self.memory) < self.capacity:\n",
        "      self.memory.append(experience)\n",
        "    else:\n",
        "      self.memory[self.push_count % self.capacity] = experience # overriding oldest exps\n",
        "    self.push_count += 1\n",
        "  \n",
        "  # to return random sample exs from RM used to train the DQN\n",
        "  def sample(self, batch_size):\n",
        "    return random.sample(self.memory, batch_size)\n",
        "\n",
        "  # check to see if RM can provide batch_size number of samples\n",
        "  def can_provide_sample(self, batch_size):\n",
        "    return len(self.memory) >= batch_size\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3qpIW18BK72",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Epsilon-greedy strategy class\n",
        "\n",
        "class EpsilonGreedyStrategy():\n",
        "  \n",
        "  def __init__(self, min, max, decay): # start, end and decay values of epsilon\n",
        "    self.min = min\n",
        "    self.max = max\n",
        "    self.decay = decay\n",
        "  \n",
        "  def get_exploration_rate(self, current_step):\n",
        "    return self.max + (self.max-self.min) * math.exp(-1*self.decay*current_step)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7F40-Y6EpmC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# RL agent\n",
        "\n",
        "class Agent():\n",
        "  \n",
        "  # initialise all the variables below\n",
        "  def __init__(self, strategy, num_actions, device): # num_actions is 2 ie, Left and Right\n",
        "    self.strategy = strategy\n",
        "    self.num_actions = num_actions\n",
        "    self.current_step = 0\n",
        "    self.device = device # device that we tell PyTorch to use for Tensor calculations ie, CPU or GPU\n",
        "\n",
        "  def select_action(self, state, policy_net): # policy_net is DQN policy we train to learn the optimal policy\n",
        "    explor_rate = strategy.get_exploration_rate(self.current_step)\n",
        "    self.current_step += 1\n",
        "\n",
        "    if self.explor_rate > random.random(): # explore\n",
        "      action = random.randrange(self.num_actions) \n",
        "      return torch.tensor([action]).to(device)\n",
        "    else:  # exploit (taking action of highest q-value ouput for the corr state)\n",
        "      with torch.no_grad(): # to turn off gradient tracking since we are using this model just for inference and not training\n",
        "        return policy_net(state).argmax(dim=1).to(device)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvLM0e_9LG2w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Environment manager\n",
        "\n",
        "class CartPoleEnvManager ():\n",
        "  def __init__(self, device):\n",
        "    self.device = device\n",
        "    self.env = gym.make('CartPole-v0').unwrapped\n",
        "    self.env.reset()\n",
        "    self.current_screen = None # we are at start of an episode where we havent yet rendered the screen\n",
        "    self.done = False # to check if any action taken has ended in the episode\n",
        "\n",
        "  def reset(self):\n",
        "    self.env.reset()\n",
        "    self.current_screen = None\n",
        "\n",
        "  def close(self):\n",
        "    self.env.close()\n",
        "\n",
        "  def render(self, mode = \"human\"):\n",
        "    return self.env.render(mode)\n",
        "  \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}