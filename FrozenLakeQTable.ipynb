{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FrozenLakeQTable.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNw1XjEBuujw1y/cKWSN90I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KGF2/DeepLearningAmateur/blob/Test/FrozenLakeQTable.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdR8Txauw9dx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "86104d28-ec02-4d19-ea2e-602e7c730c11"
      },
      "source": [
        "# FROzen lake Q-table\n",
        "\n",
        "!pip install -q sklearn\n",
        "import numpy as np\n",
        "import gym\n",
        "import random\n",
        "import time\n",
        "from IPython.display import clear_output \n",
        "\n",
        "#setting up the environment\n",
        "env = gym.make(\"FrozenLake-v0\")\n",
        "\n",
        "#initializing q-table\n",
        "action_space_size = env.action_space.n\n",
        "state_space_size = env.observation_space.n\n",
        "q_table = np.zeros((state_space_size, action_space_size))\n",
        "print(env.action_space, env.observation_space)\n",
        "print(action_space_size, state_space_size)\n",
        "print(q_table)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Discrete(4) Discrete(16)\n",
            "4 16\n",
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-70VJjhDx1z-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#algorithm parameters\n",
        "num_episodes = 5\n",
        "max_steps_per_episode = 6\n",
        "\n",
        "learning_rate = 0.1 # alpha\n",
        "discount_rate = 0.99 # gamma\n",
        "\n",
        "exploration_rate = 0.5\n",
        "max_exploration_rate = 1\n",
        "min_exploration_rate = 0.01\n",
        "exploration_decay_rate = 0.001\n",
        "\n",
        "rewards_all_episodes = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYKKR7vbyfaS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for episodes in range (num_episodes):\n",
        "  \n",
        "  state = env.reset()\n",
        "  max_steps_per_episode = 0\n",
        "  rewards_current_episode = 0\n",
        "  done = False\n",
        "\n",
        "  for steps in range (max_steps_per_episode):\n",
        "    exploration_rate_threshold = random.uniform(0,1)\n",
        "    print (exploration_rage_threshold)\n",
        "    if exploration_rate > exploration_rate_threshold:\n",
        "      action = np.argmax(q_table[state,:])\n",
        "    else:\n",
        "      action = env.action_space.sample()\n",
        "\n",
        "    new_state, reward, done, info = env.step(action)\n",
        "    q_table[state,action] = q_table[state,action] + reward*np.max(q_table[new_state,:])\n",
        "    state = new_state\n",
        "    rewards_current_episode += reward\n",
        "    print(new_state, state, action, reward, rewards_current_episode)\n",
        "    #print(q_table)\n",
        "  if done == True:\n",
        "    break\n",
        "rewards_all_episodes.append(rewards_current_episode)\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mW3-LL8LDKvS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "outputId": "8d812a7c-9448-4046-a296-d5d8a67b959e"
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import random\n",
        "import time\n",
        "from IPython.display import clear_output \n",
        "\n",
        "#setting up the environment\n",
        "env = gym.make(\"FrozenLake-v0\")\n",
        "\n",
        "#initializing q-table\n",
        "action_space_size = env.action_space.n\n",
        "state_space_size = env.observation_space.n\n",
        "q_table = np.zeros((state_space_size, action_space_size))\n",
        "print(q_table)\n",
        "\n",
        "#algorithm parameters\n",
        "num_episodes = 10000\n",
        "max_steps_per_episode = 100\n",
        "\n",
        "learning_rate = 0.1 # alpha\n",
        "discount_rate = 0.99 # gamma\n",
        "\n",
        "exploration_rate = 1\n",
        "max_exploration_rate = 1\n",
        "min_exploration_rate = 0.01\n",
        "exploration_decay_rate = 0.001\n",
        "\n",
        "rewards_all_episodes = []\n",
        "\n",
        "# complete q-learning algorithm where training takes place\n",
        "\n",
        "for episode in range (num_episodes):\n",
        "    \n",
        "    state = env.reset() # resetting the state of the env\n",
        "    done = False # to keep track of whether episode is finished or not\n",
        "    rewards_current_episode = 0 # to keep track of rewards in current episode\n",
        "    \n",
        "    for step in range(max_steps_per_episode): # everyting that happens in each time-step within each episode\n",
        "        \n",
        "        # exploration-exploitation trade-off\n",
        "        exploration_rate_threshold = random.uniform(0,1) # choosing a random number between 0 to 1\n",
        "        \n",
        "        if exploration_rate_threshold > exploration_rate : # in this if-else block agent's action will be choosen\n",
        "            action = np.argmax(q_table[state,:]) # agent will exploit the env and hence chose the action that has highest Q-value in the current state\n",
        "        else:\n",
        "            action = env.action_space.sample() # agent will explore the env and sample an action, randomly\n",
        "                \n",
        "        new_state, reward, done, info = env.step(action) # we take the choosen action by calling step on the env object, and pass the action to it. \n",
        "        # step returns a tuple containing all the above 4 variable values.\n",
        "        # print(\"action\", action, state, new_state, reward, done)\n",
        "\n",
        "        # update q-table for Q(s,a)\n",
        "        q_table[state, action] = ((1-learning_rate) * q_table[state, action]) + (learning_rate * (reward + discount_rate * (np.max(q_table[new_state,:]))))\n",
        "        # new q-value for the state-action pair in LHS is a weighted sum of old value and learned value ie, \n",
        "        # q(s,a) = [(1-alpha) * q(s,a)] + [(alpha * q(s', a'))]\n",
        "        #print(\"np.max(q_table[new_state is ...\", np.max(q_table[new_state,:]), learning_rate, discount_rate, reward)\n",
        "        \n",
        "        state = new_state # updating the current state to new state\n",
        "        rewards_current_episode += reward # adding reward obtained from this step, to that of the rewards in current episode\n",
        "        \n",
        "        if done == True: # to check if agent stepped in the Hole of reached the Goal\n",
        "            break\n",
        "            \n",
        "    # Exploration rate decay, exponential decay after each episode \n",
        "    exploration_rate = min_exploration_rate + ((max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode))\n",
        "    \n",
        "    rewards_all_episodes.append(rewards_current_episode) # append list of rewards from current episode, to that of all episodes\n",
        "    \n",
        "# calculate and print average rewards per 1000 episodes\n",
        "rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes),num_episodes/5000)\n",
        "count = 5000\n",
        "#print(\"**********************Average reward per 1000 episodes********************\\n\")\n",
        "for r in rewards_per_thousand_episodes:\n",
        "    print(count, \": \", str(sum(r/5000)))\n",
        "    count += 5000\n",
        "    \n",
        "# Print updated Q-table\n",
        "print(\"\\n\\n**********************Updated Q-TABLE ********************\\n\")\n",
        "print(q_table)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n",
            "5000 :  0.3763999999999912\n",
            "10000 :  0.6693999999999589\n",
            "\n",
            "\n",
            "**********************Updated Q-TABLE ********************\n",
            "\n",
            "[[0.55518748 0.51662382 0.52324627 0.52414767]\n",
            " [0.37663298 0.32809961 0.41130668 0.51063308]\n",
            " [0.420557   0.43474533 0.3851635  0.48767427]\n",
            " [0.21435339 0.32026624 0.28483754 0.46716346]\n",
            " [0.56762964 0.5439071  0.4360251  0.30565113]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.22799794 0.21323504 0.39546059 0.13851528]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.47804917 0.47291674 0.38636734 0.60578268]\n",
            " [0.36190543 0.63630907 0.31073583 0.44065767]\n",
            " [0.66756142 0.42705466 0.19400574 0.29914897]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.47646803 0.6618502  0.76963089 0.41650256]\n",
            " [0.7292864  0.89405486 0.76595699 0.74954801]\n",
            " [0.         0.         0.         0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUJZ6IhhKykF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "af6dc0bf-63de-4d0b-a2b0-cf213fa104e1"
      },
      "source": [
        "# test / play\n",
        "for episode in range(3):\n",
        "  state = env.reset()\n",
        "  done = False\n",
        "  print(\"*********** EPISODE : \", episode+1, \"************ \\n\\n\\n\\n\")\n",
        "  time.sleep(2)\n",
        "\n",
        "  for steps in range(max_steps_per_episode):\n",
        "    clear_output(wait=True)\n",
        "    env.render()\n",
        "    time.sleep(3)\n",
        "\n",
        "    action = np.argmax(q_table[state,:])\n",
        "    new_state, reward, done, info = env.step(action)\n",
        "\n",
        "    if done:\n",
        "      clear_output(wait=True)\n",
        "      env.render()\n",
        "      if reward==1:\n",
        "        print(\"GOAL REACHED :)\")\n",
        "      else:\n",
        "        print(\"Dead!!! :'(\")\n",
        "      time.sleep(3)\n",
        "      clear_output(wait=True)\n",
        "      break\n",
        "\n",
        "    state = new_state\n",
        "\n",
        "env.close()\n",
        "\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "GOAL REACHED :)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}