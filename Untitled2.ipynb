{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNkJlVCSHND1oXEhvoa95p2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KGF2/DeepLearningAmateur/blob/master/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdR8Txauw9dx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "752b0a60-7ca9-4456-a599-af500bddb89c"
      },
      "source": [
        "\n",
        "!pip install -q sklearn\n",
        "import numpy as np\n",
        "import gym\n",
        "import random\n",
        "import time\n",
        "from IPython.display import clear_output \n",
        "\n",
        "#setting up the environment\n",
        "env = gym.make(\"FrozenLake-v0\")\n",
        "\n",
        "#initializing q-table\n",
        "action_space_size = env.action_space.n\n",
        "state_space_size = env.observation_space.n\n",
        "q_table = np.zeros((state_space_size, action_space_size))\n",
        "print(env.action_space, env.observation_space)\n",
        "print(action_space_size, state_space_size)\n",
        "print(q_table)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Discrete(4) Discrete(16)\n",
            "4 16\n",
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-70VJjhDx1z-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#algorithm parameters\n",
        "num_episodes = 5\n",
        "max_steps_per_episode = 6\n",
        "\n",
        "learning_rate = 0.1 # alpha\n",
        "discount_rate = 0.99 # gamma\n",
        "\n",
        "exploration_rate = 0.5\n",
        "max_exploration_rate = 1\n",
        "min_exploration_rate = 0.01\n",
        "exploration_decay_rate = 0.001\n",
        "\n",
        "rewards_all_episodes = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYKKR7vbyfaS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for episodes in range (num_episodes):\n",
        "  \n",
        "  state = env.reset()\n",
        "  max_steps_per_episode = 0\n",
        "  rewards_current_episode = 0\n",
        "  done = False\n",
        "\n",
        "  for steps in range (max_steps_per_episode):\n",
        "    exploration_rate_threshold = random.uniform(0,1)\n",
        "    print (exploration_rage_threshold)\n",
        "    if exploration_rate > exploration_rate_threshold:\n",
        "      action = np.argmax(q_table[state,:])\n",
        "    else:\n",
        "      action = env.action_space.sample()\n",
        "\n",
        "    new_state, reward, done, info = env.step(action)\n",
        "    q_table[state,action] = q_table[state,action] + reward*np.max(q_table[new_state,:])\n",
        "    state = new_state\n",
        "    rewards_current_episode += reward\n",
        "    print(new_state, state, action, reward, rewards_current_episode)\n",
        "    #print(q_table)\n",
        "  if done == True:\n",
        "    break\n",
        "rewards_all_episodes.append(rewards_current_episode)\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mW3-LL8LDKvS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "outputId": "0c6777f7-a845-4976-b4a8-4fedde7e7b8b"
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import random\n",
        "import time\n",
        "from IPython.display import clear_output \n",
        "\n",
        "#setting up the environment\n",
        "env = gym.make(\"FrozenLake-v0\")\n",
        "\n",
        "#initializing q-table\n",
        "action_space_size = env.action_space.n\n",
        "state_space_size = env.observation_space.n\n",
        "q_table = np.zeros((state_space_size, action_space_size))\n",
        "print(q_table)\n",
        "\n",
        "#algorithm parameters\n",
        "num_episodes = 10000\n",
        "max_steps_per_episode = 100\n",
        "\n",
        "learning_rate = 0.1 # alpha\n",
        "discount_rate = 0.99 # gamma\n",
        "\n",
        "exploration_rate = 1\n",
        "max_exploration_rate = 1\n",
        "min_exploration_rate = 0.01\n",
        "exploration_decay_rate = 0.001\n",
        "\n",
        "rewards_all_episodes = []\n",
        "\n",
        "# complete q-learning algorithm where training takes place\n",
        "\n",
        "for episode in range (num_episodes):\n",
        "    \n",
        "    state = env.reset() # resetting the state of the env\n",
        "    done = False # to keep track of whether episode is finished or not\n",
        "    rewards_current_episode = 0 # to keep track of rewards in current episode\n",
        "    \n",
        "    for step in range(max_steps_per_episode): # everyting that happens in each time-step within each episode\n",
        "        \n",
        "        # exploration-exploitation trade-off\n",
        "        exploration_rate_threshold = random.uniform(0,1) # choosing a random number between 0 to 1\n",
        "        \n",
        "        if exploration_rate_threshold > exploration_rate : # in this if-else block agent's action will be choosen\n",
        "            action = np.argmax(q_table[state,:]) # agent will exploit the env and hence chose the action that has highest Q-value in the current state\n",
        "        else:\n",
        "            action = env.action_space.sample() # agent will explore the env and sample an action, randomly\n",
        "                \n",
        "        new_state, reward, done, info = env.step(action) # we take the choosen action by calling step on the env object, and pass the action to it. \n",
        "        # step returns a tuple containing all the above 4 variable values.\n",
        "        # print(\"action\", action, state, new_state, reward, done)\n",
        "\n",
        "        # update q-table for Q(s,a)\n",
        "        q_table[state, action] = ((1-learning_rate) * q_table[state, action]) + (learning_rate * (reward + discount_rate * (np.max(q_table[new_state,:]))))\n",
        "        # new q-value for the state-action pair in LHS is a weighted sum of old value and learned value ie, \n",
        "        # q(s,a) = [(1-alpha) * q(s,a)] + [(alpha * q(s', a'))]\n",
        "        #print(\"np.max(q_table[new_state is ...\", np.max(q_table[new_state,:]), learning_rate, discount_rate, reward)\n",
        "        \n",
        "        state = new_state # updating the current state to new state\n",
        "        rewards_current_episode += reward # adding reward obtained from this step, to that of the rewards in current episode\n",
        "        \n",
        "        if done == True: # to check if agent stepped in the Hole of reached the Goal\n",
        "            break\n",
        "            \n",
        "    # Exploration rate decay, exponential decay after each episode \n",
        "    exploration_rate = min_exploration_rate + ((max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode))\n",
        "    \n",
        "    rewards_all_episodes.append(rewards_current_episode) # append list of rewards from current episode, to that of all episodes\n",
        "    \n",
        "# calculate and print average rewards per 1000 episodes\n",
        "rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes),num_episodes/5000)\n",
        "count = 5000\n",
        "#print(\"**********************Average reward per 1000 episodes********************\\n\")\n",
        "for r in rewards_per_thousand_episodes:\n",
        "    print(count, \": \", str(sum(r/5000)))\n",
        "    count += 5000\n",
        "    \n",
        "# Print updated Q-table\n",
        "print(\"\\n\\n**********************Updated Q-TABLE ********************\\n\")\n",
        "print(q_table)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n",
            "5000 :  0.36599999999999233\n",
            "10000 :  0.6801999999999577\n",
            "\n",
            "\n",
            "**********************Updated Q-TABLE ********************\n",
            "\n",
            "[[0.53845215 0.51974414 0.5224312  0.51619985]\n",
            " [0.29847431 0.28571278 0.30404798 0.49083295]\n",
            " [0.41991275 0.40082324 0.40702319 0.46888805]\n",
            " [0.28778818 0.2714912  0.22699709 0.45216544]\n",
            " [0.54789063 0.337288   0.43055133 0.44678286]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.3384611  0.20739245 0.17479086 0.06630789]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.47779882 0.40491517 0.47546934 0.56966242]\n",
            " [0.33380882 0.65388906 0.42603213 0.34802937]\n",
            " [0.65687861 0.37451689 0.42069099 0.34283434]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.46129035 0.57890582 0.73645603 0.52865979]\n",
            " [0.7637522  0.90078947 0.75960906 0.76034053]\n",
            " [0.         0.         0.         0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}