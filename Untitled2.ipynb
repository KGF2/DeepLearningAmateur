{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPvog6up6GQPp4rMfoYADpe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KGF2/DeepLearningAmateur/blob/master/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdR8Txauw9dx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "752b0a60-7ca9-4456-a599-af500bddb89c"
      },
      "source": [
        "\n",
        "!pip install -q sklearn\n",
        "import numpy as np\n",
        "import gym\n",
        "import random\n",
        "import time\n",
        "from IPython.display import clear_output \n",
        "\n",
        "#setting up the environment\n",
        "env = gym.make(\"FrozenLake-v0\")\n",
        "\n",
        "#initializing q-table\n",
        "action_space_size = env.action_space.n\n",
        "state_space_size = env.observation_space.n\n",
        "q_table = np.zeros((state_space_size, action_space_size))\n",
        "print(env.action_space, env.observation_space)\n",
        "print(action_space_size, state_space_size)\n",
        "print(q_table)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Discrete(4) Discrete(16)\n",
            "4 16\n",
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-70VJjhDx1z-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#algorithm parameters\n",
        "num_episodes = 5\n",
        "max_steps_per_episode = 6\n",
        "\n",
        "learning_rate = 0.1 # alpha\n",
        "discount_rate = 0.99 # gamma\n",
        "\n",
        "exploration_rate = 0.5\n",
        "max_exploration_rate = 1\n",
        "min_exploration_rate = 0.01\n",
        "exploration_decay_rate = 0.001\n",
        "\n",
        "rewards_all_episodes = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYKKR7vbyfaS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for episodes in range (num_episodes):\n",
        "  \n",
        "  state = env.reset()\n",
        "  max_steps_per_episode = 0\n",
        "  rewards_current_episode = 0\n",
        "  done = False\n",
        "\n",
        "  for steps in range (max_steps_per_episode):\n",
        "    exploration_rate_threshold = random.uniform(0,1)\n",
        "    print (exploration_rage_threshold)\n",
        "    if exploration_rate > exploration_rate_threshold:\n",
        "      action = np.argmax(q_table[state,:])\n",
        "    else:\n",
        "      action = env.action_space.sample()\n",
        "\n",
        "    new_state, reward, done, info = env.step(action)\n",
        "    q_table[state,action] = q_table[state,action] + reward*np.max(q_table[new_state,:])\n",
        "    state = new_state\n",
        "    rewards_current_episode += reward\n",
        "    print(new_state, state, action, reward, rewards_current_episode)\n",
        "    #print(q_table)\n",
        "  if done == True:\n",
        "    break\n",
        "rewards_all_episodes.append(rewards_current_episode)\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mW3-LL8LDKvS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "outputId": "c27e889f-631e-499e-e8a9-49391200349d"
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import random\n",
        "import time\n",
        "from IPython.display import clear_output \n",
        "\n",
        "#setting up the environment\n",
        "env = gym.make(\"FrozenLake-v0\")\n",
        "\n",
        "#initializing q-table\n",
        "action_space_size = env.action_space.n\n",
        "state_space_size = env.observation_space.n\n",
        "q_table = np.zeros((state_space_size, action_space_size))\n",
        "print(q_table)\n",
        "\n",
        "#algorithm parameters\n",
        "num_episodes = 10000\n",
        "max_steps_per_episode = 100\n",
        "\n",
        "learning_rate = 0.1 # alpha\n",
        "discount_rate = 0.99 # gamma\n",
        "\n",
        "exploration_rate = 1\n",
        "max_exploration_rate = 1\n",
        "min_exploration_rate = 0.01\n",
        "exploration_decay_rate = 0.001\n",
        "\n",
        "rewards_all_episodes = []\n",
        "\n",
        "# complete q-learning algorithm where training takes place\n",
        "\n",
        "for episode in range (num_episodes):\n",
        "    \n",
        "    state = env.reset() # resetting the state of the env\n",
        "    done = False # to keep track of whether episode is finished or not\n",
        "    rewards_current_episode = 0 # to keep track of rewards in current episode\n",
        "    \n",
        "    for step in range(max_steps_per_episode): # everyting that happens in each time-step within each episode\n",
        "        \n",
        "        # exploration-exploitation trade-off\n",
        "        exploration_rate_threshold = random.uniform(0,1) # choosing a random number between 0 to 1\n",
        "        \n",
        "        if exploration_rate_threshold > exploration_rate : # in this if-else block agent's action will be choosen\n",
        "            action = np.argmax(q_table[state,:]) # agent will exploit the env and hence chose the action that has highest Q-value in the current state\n",
        "        else:\n",
        "            action = env.action_space.sample() # agent will explore the env and sample an action, randomly\n",
        "                \n",
        "        new_state, reward, done, info = env.step(action) # we take the choosen action by calling step on the env object, and pass the action to it. \n",
        "        # step returns a tuple containing all the above 4 variable values.\n",
        "        # print(\"action\", action, state, new_state, reward, done)\n",
        "\n",
        "        # update q-table for Q(s,a)\n",
        "        q_table[state, action] = ((1-learning_rate) * q_table[state, action]) + (learning_rate * (reward + discount_rate * (np.max(q_table[new_state,:]))))\n",
        "        # new q-value for the state-action pair in LHS is a weighted sum of old value and learned value ie, \n",
        "        # q(s,a) = [(1-alpha) * q(s,a)] + [(alpha * q(s', a'))]\n",
        "        #print(\"np.max(q_table[new_state is ...\", np.max(q_table[new_state,:]), learning_rate, discount_rate, reward)\n",
        "        \n",
        "        state = new_state # updating the current state to new state\n",
        "        rewards_current_episode += reward # adding reward obtained from this step, to that of the rewards in current episode\n",
        "        \n",
        "        if done == True: # to check if agent stepped in the Hole of reached the Goal\n",
        "            break\n",
        "            \n",
        "    # Exploration rate decay, exponential decay after each episode \n",
        "    exploration_rate = min_exploration_rate + ((max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode))\n",
        "    \n",
        "    rewards_all_episodes.append(rewards_current_episode) # append list of rewards from current episode, to that of all episodes\n",
        "    \n",
        "# calculate and print average rewards per 1000 episodes\n",
        "rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes),num_episodes/5000)\n",
        "count = 5000\n",
        "#print(\"**********************Average reward per 1000 episodes********************\\n\")\n",
        "for r in rewards_per_thousand_episodes:\n",
        "    print(count, \": \", str(sum(r/5000)))\n",
        "    count += 5000\n",
        "    \n",
        "# Print updated Q-table\n",
        "print(\"\\n\\n**********************Updated Q-TABLE ********************\\n\")\n",
        "print(q_table)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n",
            "5000 :  0.3699999999999919\n",
            "10000 :  0.6855999999999571\n",
            "\n",
            "\n",
            "**********************Updated Q-TABLE ********************\n",
            "\n",
            "[[0.54835366 0.51593931 0.51491481 0.51451823]\n",
            " [0.34815884 0.36208451 0.32463517 0.49452731]\n",
            " [0.40755388 0.40980498 0.42279297 0.46835912]\n",
            " [0.25683394 0.34686158 0.27961199 0.46040683]\n",
            " [0.57251092 0.36946792 0.28278958 0.46566963]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.34345256 0.14189797 0.21106913 0.10464156]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.29806798 0.51573292 0.2858804  0.61497888]\n",
            " [0.51133987 0.65328354 0.46858438 0.43811451]\n",
            " [0.62409058 0.34249596 0.39097995 0.32097911]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.3134962  0.56164077 0.79483623 0.50695738]\n",
            " [0.7317569  0.86645902 0.72291074 0.75536158]\n",
            " [0.         0.         0.         0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUJZ6IhhKykF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "f8de46eb-d252-4938-b9db-1435df835074"
      },
      "source": [
        "# test / play\n",
        "for episode in range(3):\n",
        "  state = env.reset()\n",
        "  done = False\n",
        "  print(\"*********** EPISODE : \", episode+1, \"************ \\n\\n\\n\\n\")\n",
        "  time.sleep(2)\n",
        "\n",
        "  for steps in range(max_steps_per_episode):\n",
        "    clear_output(wait=True)\n",
        "    env.render()\n",
        "    time.sleep(0.3)\n",
        "\n",
        "    action = np.argmax(q_table[state,:])\n",
        "    new_state, reward, done, info = env.step(action)\n",
        "\n",
        "    if done:\n",
        "      clear_output(wait=True)\n",
        "      env.render()\n",
        "      if reward==1:\n",
        "        print(\"GOAL REACHED :)\")\n",
        "      else:\n",
        "        print(\"Dead!!! :'(\")\n",
        "      time.sleep(3)\n",
        "      clear_output(wait=True)\n",
        "      break\n",
        "\n",
        "    state = new_state\n",
        "\n",
        "env.close()\n",
        "\n",
        "\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "GOAL REACHED :)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}